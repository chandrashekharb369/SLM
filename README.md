# ğŸ§  SLM: Specialist Language Models with Custom Activation

![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?logo=pytorch&logoColor=white)
![HuggingFace](https://img.shields.io/badge/HuggingFace-Transformers-yellow?logo=huggingface)
![Status](https://img.shields.io/badge/Status-Active-brightgreen)

An end-to-end Python project to build, train, and interact with **Specialist Language Models (SLMs)**, designed to run efficiently on consumer-grade hardware.  
The **core innovation** is a custom, computationally fast activation function.

---

## ğŸŒŸ Vision

Instead of chasing massive general-purpose models, SLMs prove that **smaller, expert-trained models** can perform better on focused tasks â€” **efficient, scalable, and democratized AI for everyone**.

ğŸ”¹ **Two initial specialists**:  
- ğŸ‘¨â€ğŸ’» **Coder Model** â†’ Built for programming-related tasks.  
- ğŸ§® **Mathematician Model** â†’ Optimized for solving math problems.  

---

## âœ¨ Key Features

- âš¡ **Custom SLiQ Activation Function** â€“ Scaled Linear Quadratic Unit for faster, stable training.  
- ğŸ§  **Specialist Architectures** â€“ Focused models instead of bloated generalists.  
- ğŸ’» **Runs on Consumer GPUs** â€“ Optimized to train on GPUs like **RTX 3050 (4GB)**.  
- ğŸ”„ **Full Pipeline** â€“ Data prep â†’ Tokenization â†’ Training â†’ Checkpointing â†’ Chatbot Inference.  

---

## ğŸ› ï¸ Tech Stack

- **Python 3.10+**  
- **PyTorch** (Deep Learning)  
- **Hugging Face Transformers** (Tokenizer & Models)  
- **Hugging Face Datasets** (Data Handling)  
- **Accelerate** (Mixed-precision training)  

---

## ğŸš€ Quickstart

### 1ï¸âƒ£ Setup

```bash
# Clone the repo
git clone https://github.com/your-username/SLM_Project.git
cd SLM_Project

# Install requirements
pip install -r requirements.txt
````

### 2ï¸âƒ£ Train Your Model

Configure everything in `src/config.py` and run:

```bash
python -m src.train
```

ğŸ‘‰ Saves checkpoints into `saved_models/`.

### 3ï¸âƒ£ Chat With Your Model

```bash
# Example: Coder Model
python -m src.chat --model coder

# Example: General Model
python -m src.chat --model general
```

---

## ğŸ“‚ Project Structure

```
SLM_Project/
â”‚
â”œâ”€â”€ saved_models/       # Trained models & checkpoints
â”œâ”€â”€ tokenizer/          # Tokenizer files
â”‚
â”œâ”€â”€ README.md           # Project overview
â”œâ”€â”€ requirements.txt    # Dependencies
â”‚
â””â”€â”€ src/                # Source code
    â”œâ”€â”€ config.py       # All configs
    â”œâ”€â”€ data_loader.py  # Data loading pipeline
    â”œâ”€â”€ model.py        # Model + SLiQ activation
    â”œâ”€â”€ train.py        # Training script
    â””â”€â”€ chat.py         # Inference/chat script
```

---

## ğŸ“Š Roadmap

* [x] Implement **SLiQ Activation Function**
* [x] Build **Coder & Math Models**
* [ ] Add **more domain specialists** (Science, Finance, Healthcare)
* [ ] Optimize inference for **mobile/edge devices**


